---
params:
  BaseDir: "Z:/"
title: "Well Variance"
author: "Marlin"
date: "11/15/2021"
output: html_document
---

```{r set up markdown settings, echo=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE
)
```


```{r Start enviroment, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(lmtest)
library(lubridate)
library(limma)
library(tidyr)
library(plotly)
  #library(lintr)


#Data Files and prep work
source("../../../lib/DataProccess.R")
```



```{r DF Set Up, echo=FALSE}
BaseDir <- params$BaseDir#get the root of the directory where the data is stored

source("../../../lib/DataPathName.R")

HFGDF <- ParseData(HFGWastePath(BaseDir))%>%#Reads in preprocessed data into consistent format
  select(Site,Date,Filter,Well,N1)%>%
  filter(!is.na(N1))%>%
  mutate(Filter = as.factor(Filter), N1 = as.numeric(N1))

tail(HFGDF)#Quick look into structure of data frame
```

Lets start by attempting to remove all other sources of error by subtracting the mean of each filter from each bunch. We use the log difference so we are dealing with normally distributed data. The resulting error is normaly distributed and its relationship with N1 is minimal.
```{r Initial distribution of errors }

CompDF <- HFGDF%>% 
  group_by(Site,Date,Filter)%>% #group by three columns that isolate a set of technical replicates
  mutate(MeanN1 = mean(N1),
      Error = N1/MeanN1) #Taking the log mean to preserve normal distribution

CompDF%>% #Plot of the shape of the errors
  ggplot() + 
  aes(x=Error) +
  geom_histogram()+#Normal distribution is a good sign that the error model is correct
  facet_wrap(~Site)


CompDF%>%
  ggplot() +
  geom_point(aes(x=MeanN1,y=Error) , size=1)+
  scale_x_log10()+
  #Using logN1 because that is what we used to generate the errors
  facet_wrap(~Site)#looking for relationship between error and N1. Should be flat
```


We can control for filter to filter difference in variance by taking the variance of the errors for each filter. The resulting data is log normal and related to N1 stronger then the errors did.
```{r sample variance}

CompDF2 <- CompDF%>% #exploring measured variance of three replicates
  group_by(Site,Date,Filter)%>%
  summarize(TechnicalVariance = var(Error), 
            MeanN1 = (mean(N1)),N=n())%>%
  filter(N!=1)#variance cant be calculated from one sample



CompDF2%>% #looking at shape of Variance. Surprising it is log normal as it was
  ggplot() + #generated by taking log N1 already.
  aes(x=log(TechnicalVariance)) +
  geom_histogram(bins = 15) +
  facet_wrap(~Site)



CompDF2%>%#Relation between logN1 and the measured variance
  ggplot() + 
  aes(y=log(TechnicalVariance)) + 
  geom_point(aes(x=MeanN1)) +
  scale_x_log10()+
  facet_wrap(~Site)
```

From this we can calculate the variance to get a handle on this. We can either take the variance of the errors or we can take the mean of the sample variances. Both give similar answers and which one is better depends on what assumptions we want to make about the errors.

```{r}

VarianceError <- var(CompDF$Error,na.rm=TRUE) # calculate variance of all the errors.
#Assumes same mean for all errors. This should be true as the value is generated
#by subtracting the mean from the original data.

MeanVariance <- weighted.mean(CompDF2$TechnicalVariance, CompDF2$N,na.rm=TRUE) #Weighted Mean
#of the variance. Does not assume each sample mean is equal but gives a larger
#variance


print(paste("Variance of Well Errors:", round(VarianceError,4)))
print(paste("weighted mean of variances", round(MeanVariance,4)))

```

