---
params:
  BaseDir: "Z:/"
title: "Trend variance"
author: "Marlin"
date: "11/15/2021"
output: html_document
---

```{r set up markdown settings, echo=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE
)
```


```{r Start enviroment, echo=FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(ggplot2)
library(lmtest)
library(lubridate)
library(limma)
library(tidyr)
library(plotly)
  #library(lintr)
  #library(psych)

#Data Files and prep work
source("../../../lib/DataProccess.R")
```



```{r DF Set Up, echo=FALSE}
BaseDir <- params$BaseDir#get the root of the directory where the data is stored

#UpdatedHFGFN.csv
#HFGWasteData071421.csv

LIMSFN <- paste0(BaseDir, "COVID-19_WastewaterAnalysis/data/processed/LIMSWasteData_2021-06-30_17-40.csv")
HFGWasteFN <-  paste0(BaseDir,"COVID-19_WastewaterAnalysis/data/processed/HFGWasteData071421.csv")
HFGCasesFN  <-  paste0(BaseDir,"COVID-19_WastewaterAnalysis/data/processed/HFGCaseData_2021-05-07.csv")

HFGCaseDF <- ParseData(HFGCasesFN)#Reads in preprocessed data into consistent format

HFGDF <- ParseData(HFGWasteFN)%>%
  select(Site,Date,Filter,Well,HFGN1=N1)

StartHFG <- min(HFGDF$Date)#DateRange that HFG analysis makes sense in
EndHFG <- max(HFGDF$Date)#used for graphing ranges later


HFGLocations <- unique(HFGDF$Site)


#Importing the LIMS waste water data
LIMSHFGLocationsDF <- ParseData(LIMSFN)%>%
  filter(Site %in% HFGLocations)%>%#Filter LIMS data to only the Sites we care about
  select(Date, Site,  N1, N1Error)

#joining the two data frames together
FullDF <- full_join(LIMSHFGLocationsDF,HFGDF, by = c("Date","Site"))%>%
  mutate(Filter=as.factor(Filter))

#what does the data look like? 
tail(FullDF)
```


LIMS N1 agrees with HFG data implying that using it to find an underlying trend of the High frequency data should be faithful.

```{r LIMS HFG data relation}
CompFullDF <- FullDF%>%
  filter(Date > StartHFG - 14,
          Date < EndHFG + 14,
          HFGN1 < 1e7)


CompN1DF <- CompFullDF%>%
  filter(!is.na(N1))

CompHFGDF <- CompFullDF%>%
  filter(!is.na(HFGN1))



CompPlot <- ggplot()+
  aes(x=Date)+
  geom_point(aes(y = HFGN1,color = Filter) , size = .1 , data = CompHFGDF) +
  geom_line(aes(y = N1,color = "Lims Data") , size = 1 , data = CompN1DF) +
  scale_y_log10() +
  facet_wrap(~Site) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

ggplotly(CompPlot)
```

using smoothing found in the main story on the N1 data.

```{r }
GroupLoess <- function(SmoothedSite){
  FilteredDF <- FullDF%>%
    filter(Site == SmoothedSite)%>%
    select(Site,Date,N1)
  FilteredDF$loessN1 <- loessFit(y=(FilteredDF$N1), 
                      x=FilteredDF$Date, #create loess fit of the data
                      span=.2, #span of .2 seems to give the best result, not rigorously chosen
                      iterations=2)$fitted#2 iterations remove some bad patterns
  return(FilteredDF)
}

FullLoessData <- bind_rows(lapply(HFGLocations,GroupLoess))

FullDF2 <- inner_join(HFGDF , FullLoessData , by = c("Date","Site"))%>%
  filter(!is.na(loessN1))%>%
  mutate(Filter = as.factor(Filter))


CompPlot2 <-FullDF2%>%
  filter(HFGN1<4e7)%>%
  ggplot()+
  aes(x=Date)+
  geom_point(aes(y = HFGN1,color=Filter) , size=.1)+
  geom_line(aes(y = loessN1) , size=1)+
  scale_y_log10()+
  facet_wrap(~Site) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

ggplotly(CompPlot2)

```



With the small amount of case data in the region the smoothing seems to generalize reasonably.

```{r LIMS HFG Smoothed relation}
CompFullDF <- FullDF2%>%
  filter(Date > StartHFG - 21,
          Date < EndHFG + 14,
          HFGN1 < 1e7)


CompN1DF <- CompFullDF%>%
  filter(!is.na(loessN1))

CompCaseHFGDF <- HFGCaseDF%>%
  #mutate(ReportedCases = ifelse(ReportedCases == -999, NA, ReportedCases))%>%
  filter(!is.na(ReportedCases))



CompPlot3 <- ggplot()+
  aes(x=Date)+
  geom_point(aes(y=ReportedCases,color="ReportedCases") , size=1 , data = CompCaseHFGDF)+
  geom_line(aes(y=loessN1/1000,color="Lims Data") , size=1 , data = CompN1DF)+
  scale_y_log10()+
  facet_wrap(~Site)+
  ggtitle("Simple shape analysis") +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

CompPlot3
```

Once we remove the trend we have a dataset of errors. The resulting error is normaly distributed and its relationship with the trend is minimal.

```{r Remove trend from data}

CompFullDF <- FullDF2%>%
  filter(HFGN1 < 1e7)%>%
  mutate(TrendError = log(HFGN1) - log(loessN1))%>%
  filter(!is.na(TrendError))


ggplot()+
  geom_histogram(aes(x=TrendError) , size=1 , data = CompFullDF)+
  facet_wrap(~Site)

ggplot()+
  geom_point(aes(x=loessN1,y=TrendError) , size=1 , data = CompFullDF)+
  facet_wrap(~Site)+
  scale_x_log10() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))

ggplot()+
  geom_point(aes(x=Date,y=TrendError) , size=1 , data = CompFullDF)+
  facet_wrap(~Site)
```

We can control for filter to filter difference in variance by taking the variance of the errors for each Day. The resulting data is log normal and does not significantly with N1.

```{r sample variance}

CompFullDF2 <- CompFullDF%>% #exploring measured variance of three replicates
  group_by(Site,Date)%>%
  summarize(TechnicalVariance = var(TrendError), LogN1 = mean(log(N1)),N=n())%>%
  filter(N!=1)#variance cant be calculated from one sample



CompFullDF2%>% #looking at shape of Variance. Surprising it is log normal as it was
  ggplot() + #generated by taking log N1 already.
  aes(x=log(TechnicalVariance)) +
  geom_histogram(bins = 15) +
  facet_wrap(~Site)



CompFullDF2%>%#Relation between logN1 and the measured variance
  ggplot() + 
  aes(y=log(TechnicalVariance)) + 
  geom_point(aes(x=LogN1)) +
  facet_wrap(~Site)

```

From this we can calculate the variance to get a handle on this. We can either take the variance of the errors or we can take the mean of the sample variances. Both give similar answers and which one is better depends on what assumptions we want to make about the errors.

```{r}

VarianceError <- var(CompFullDF$TrendError) # calculate variance of all the errors.
#Assumes same mean for all errors. This should be true as the value is generated
#by subtracting the mean from the original data.

MeanVariance <- weighted.mean(CompFullDF2$TechnicalVariance, CompFullDF2$N) #Weighted Mean
#of the variance. Does not assume each sample mean is equal but gives a larger
#variance


print(paste("Variance of Trend Errors:", round(VarianceError,4)))
print(paste("weighted mean of variances", round(MeanVariance,4)))
```










